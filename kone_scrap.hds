#coding: utf8
#title: kone.gg
#comment: https://kone.gg

import downloader
from utils import urljoin, Downloader, Soup, clean_title, get_ext, File, Session, get_print
from translator import tr_
import errors
import clf2
from io import BytesIO
import re
import json
import datetime
import traceback

class LoginRequired(errors.LoginRequired):
    def __init__(self, *args):
        super().__init__(*args, method='browser', url='https://kone.gg/account/signin?path=%2F')

def get_post_id(url):
    if '/s/' in url:
        try:
            parts = url.split('/')
            return parts[5].split('?')[0]
        except: pass
    return None

# [CSS]
KONE_CLEAN_CSS = """
    :root { --bg-primary: #18181b; --bg-secondary: #27272a; --text-primary: #e4e4e7; --text-secondary: #a1a1aa; --accent: #3b82f6; --border: #3f3f46; }
    body { margin: 0; padding: 0; background-color: var(--bg-primary); color: var(--text-primary); font-family: Pretendard, -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif; line-height: 1.6; }
    .layout { max-width: 900px; margin: 0 auto; padding: 40px 20px; min-height: 100vh; }
    header { border-bottom: 1px solid var(--border); padding-bottom: 20px; margin-bottom: 30px; }
    .badge { display: inline-block; background: var(--bg-secondary); color: var(--text-secondary); padding: 2px 8px; border-radius: 9999px; font-size: 12px; font-weight: 600; margin-bottom: 10px; border: 1px solid var(--border); }
    h1 { margin: 0 0 10px 0; font-size: 24px; font-weight: 700; line-height: 1.4; color: #fff; }
    .meta { display: flex; align-items: center; gap: 10px; font-size: 13px; color: var(--text-secondary); }
    .avatar { width: 24px; height: 24px; border-radius: 50%; background: var(--bg-secondary); display: flex; align-items: center; justify-content: center; font-weight: bold; overflow: hidden; color: #fff; font-size: 12px; }
    .prose { font-size: 16px; margin-bottom: 40px; word-break: break-all; }
    .prose img { max-width: 100%; height: auto; display: block; margin: 20px auto; border-radius: 8px; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.3); }
    .comments-section { margin-top: 50px; border-top: 1px solid var(--border); padding-top: 30px; }
    .comments-header { font-size: 18px; font-weight: 600; margin-bottom: 20px; color: #fff; }
    .comment-item { display: flex; gap: 12px; margin-bottom: 15px; padding: 12px; background-color: var(--bg-secondary); border-radius: 8px; }
    .comment-avatar { width: 32px; height: 32px; border-radius: 50%; background: #52525b; flex-shrink: 0; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 12px; color: #fff; overflow: hidden; }
    .comment-avatar img { width: 100%; height: 100%; object-fit: cover; }
    .comment-body { flex: 1; min-width: 0; }
    .comment-meta { display: flex; align-items: baseline; gap: 8px; margin-bottom: 4px; }
    .comment-author { font-size: 14px; font-weight: 600; color: #fff; }
    .comment-date { font-size: 12px; color: var(--text-secondary); }
    .comment-text { font-size: 14px; color: var(--text-primary); white-space: pre-wrap; word-break: break-word; }
"""

class Downloader_kone(Downloader):
    type = 'kone'
    URLS = ['kone.gg']
    icon = 'https://kone.gg/images/sub_icon.png'
    ACCEPT_COOKIES = [r'(.*\.)?kone\.gg']

    def init(self):
        self.session = Session()
        self.session.headers['User-Agent'] = downloader.hdr['User-Agent']
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
            'Referer': 'https://kone.gg/',
            'Origin': 'https://kone.gg',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        })

    @property
    def id(self):
        pid = get_post_id(self.url)
        return f'kone_{pid}' if pid else 'kone_unknown'

    def read(self):
        token = None
            
        # Check data stability
        js_check = """document.querySelector('#post_content').shadowRoot.querySelector('div[data-post-id]').getAttribute('data-post-id');"""

        def f(html, browser=None):
            browser.show()
            def check_cb(r): nonlocal token; token = r
            browser.runJavaScript(js_check, callback=check_cb)
            
            if token:
                browser.hide()
            return True

        res = clf2.solve(self.url, f=f, session=self.session)
        raw_html = res['html']

        self.data_post_id = token

        print_ = get_print(self.cw)
        print_(f'token: {token}')


        # 데이터 조립
        full_stream_data = ""
        scripts = re.findall(r'<script[^>]*>(.*?)</script>', raw_html, re.DOTALL)
        for script_content in scripts:
            if 'self.__next_f.push' in script_content:
                try:
                    start_idx = script_content.find('self.__next_f.push(') + len('self.__next_f.push(')
                    end_idx = script_content.rfind(')')
                    
                    if start_idx > len('self.__next_f.push(') - 1 and end_idx > start_idx:
                        json_str = script_content[start_idx:end_idx]
                        chunk = json.loads(json_str)
                        
                        if isinstance(chunk, list) and len(chunk) >= 2 and isinstance(chunk[1], str):
                            full_stream_data += chunk[1]
                except: pass

        # 데이터 추출
        post_id = get_post_id(self.url)
        if not post_id: raise Exception("게시글 ID를 찾을 수 없습니다.")

        info = {'title': 'Untitled', 'content': '', 'writer': 'Unknown', 'date': datetime.datetime.now().strftime('%Y-%m-%d')}
        found_data = False

        # PackedArticle
        if not found_data:
            try:
                key_idx = full_stream_data.find('"PackedArticle":')
                if key_idx != -1:
                    val_start = full_stream_data.find('"', key_idx + len('"PackedArticle":'))
                    if val_start != -1:
                        decoder = json.JSONDecoder()
                        packed_str, _ = decoder.raw_decode(full_stream_data, val_start)

                        article_data = json.loads(packed_str)
                        
                        art_id = article_data.get('id', {})
                        art_id_val = art_id.get('v') if isinstance(art_id, dict) else art_id
                        
                        if art_id_val == post_id:
                            info['title'] = article_data.get('title', 'Untitled')
                            info['content'] = article_data.get('content', '')
                            w_obj = article_data.get('writer', {})
                            info['writer'] = w_obj.get('display_name', 'Unknown')
                            created = article_data.get('created_at', {})
                            c_val = created.get('v') if isinstance(created, dict) else created
                            if c_val: info['date'] = c_val[:10]
                            found_data = True
            except: pass

        # 스트림 스캔
        if not found_data:
            t_match = re.search(r'"v":"' + re.escape(post_id) + r'".*?"title":"(.*?)"', full_stream_data)
            if t_match:
                try: info['title'] = json.loads(f'"{t_match.group(1)}"')
                except: info['title'] = t_match.group(1)
            
            c_match = re.search(r'"v":"' + re.escape(post_id) + r'".*?"content":"(.*?)(?<!\\)"', full_stream_data)
            if c_match:
                try: info['content'] = json.loads(f'"{c_match.group(1)}"')
                except: pass
            
            w_match = re.search(r'"v":"' + re.escape(post_id) + r'".*?"display_name":"(.*?)"', full_stream_data)
            if w_match:
                try: info['writer'] = json.loads(f'"{w_match.group(1)}"')
                except: pass
            
            d_match = re.search(r'"created_at":"(.*?)"', full_stream_data)
            if d_match: info['date'] = d_match.group(1)[:10]

        # 제목
        if info['title'] == 'Untitled':
            soup = Soup(raw_html)
            if soup.title: info['title'] = soup.title.string.split('|')[0].strip()
        self.title = clean_title(info['title'])

        # 카테고리 + 제목
        try:
            soup = Soup(raw_html)
            h1 = soup.find('h1', class_='flex')
            if h1:
                title_text = h1.get_text(strip=True)
                badge = h1.find_previous_sibling('span', attrs={'data-slot': 'badge'})
                
                if badge:
                    cat_name = badge.get_text(strip=True)
                    self.title = clean_title(f'[{cat_name}] {title_text}')
        except: pass

        # 댓글 추출
        comments = []
        try:
            comments_match = re.search(r'"Comments":(\[.*?\])', full_stream_data)
            if comments_match:
                json_comments = json.loads(comments_match.group(1))
                for c in json_comments:
                    w = c.get('writer', {}).get('display_name', 'Unknown')
                    t = c.get('content', '')
                    d = c.get('created_at', '')[:10]
                    pid = c.get('parent_id')
                    comments.append({'writer': w, 'content': t, 'date': d, 'parent_id': pid})
        except: pass

        if not comments:
            soup = Soup(raw_html)
            for item in soup.find_all('div', class_='group/comment'):
                try:
                    w_tag = item.find('a', href=re.compile(r'^/u/'))
                    w = w_tag.get_text(strip=True) if w_tag else "Unknown"
                    if w_tag and w_tag.find('img') and w_tag.find('img').get('alt'):
                        w = w_tag.find('img').get('alt')
                    c_tag = item.find('div', class_='whitespace-pre-wrap')
                    c = c_tag.get_text('\n', strip=True) if c_tag else ""
                    d_tag = item.find('div', class_='text-zinc-500')
                    d = d_tag.get_text(strip=True) if d_tag else ""
                    parent = item.find_parent('div', class_='relative')
                    is_reply = parent and ('ml-' in str(parent.get('class', [])))
                    comments.append({'writer': w, 'content': c, 'date': d, 'parent_id': is_reply})
                except: pass

        # HTML에서 이미지(열화판) 추출
        image_list = []
        seen = set()
        
        content_soup = Soup(info['content'])
        for img in content_soup.find_all('img'):
            src = img.get('src') or img.get('data-originalurl')
            if not src:
                continue
        
            full_url = urljoin(self.url, src).replace(r'\/', '/')
        
            if full_url in seen:
                continue

            seen.add(full_url)
            image_list.append(full_url)

        # 원본 이미지 URL 가져오기 시도
        original_url_imgs = []
        data_post_id = self.data_post_id
        print_ = get_print(self.cw)
        api_url = ""

        if data_post_id and image_list:
            try:
                api_url = f"https://api.kone.gg/v0/article/{data_post_id}/media/original"
                api_data = {'media_url': image_list}
                api_headers = self.session.headers.copy()
                api_headers['Content-Type'] = 'application/json'

                response = self.session.post(api_url, json=api_data, headers=api_headers, timeout=10)
                response.raise_for_status()
                media_data = response.json()
                
                original_url_imgs = [media.get('url') for media in media_data.get('media', []) if media.get('url')]
                print_(f'[Kone] 원본 이미지 추출 성공: {len(original_url_imgs)}개')
            except Exception as e:
                print_(f'[Kone] 원본 API 호출 실패: {e}')
                original_url_imgs = []
        else:
            if not data_post_id:
                print_('[Kone] data_post_id가 없으므로 열화판으로 진행합니다.')

        # URL 리스트 확정 및 파일 객체 생성
        self.urls = []
        url_to_filename = {}

        # ID가 있고, 개수가 일치할 때만 원본 모드 활성화
        use_original = data_post_id and (len(image_list) == len(original_url_imgs))

        for i, raw_url in enumerate(image_list):
            target_url = original_url_imgs[i] if use_original else raw_url
            
            clean_url = target_url
            ext = get_ext(clean_url) or '.webp'
            filename = f'{i+1:04}{ext}'
            
            file_obj = File_kone({
                'url': clean_url, 
                'name': filename, 
                'referer': api_url if use_original else self.url
            }, fallback_url=raw_url if use_original else None)

            self.urls.append(file_obj)
            
            url_to_filename[raw_url] = filename
            url_to_filename[clean_url] = filename
            url_to_filename[raw_url.replace('&', '&amp;')] = filename

            if 'mittere.io' in clean_url:
                try:
                    file_hash = clean_url.split('/')[-1]
                    url_to_filename[file_hash] = filename
                except: pass

        # HTML 생성
        html_data = {
            'title': info['title'],
            'content': info['content'],
            'writer': info['writer'],
            'date': info['date'],
            'url': self.url,
            'comments': comments,
            'url_map': url_to_filename
        }
        self.urls.append(HTML(html_data))


class File_kone(File):
    def __init__(self, info, fallback_url=None):
        super().__init__(info)
        self.fallback_url = fallback_url
        self.referer_url = info.get('referer')

    def alter(self):
        if self.fallback_url: return self.fallback_url
        return self.referer_url

class HTML(File):
    type = 'kone'
    format = 'title'

    def __init__(self, info):
        title = clean_title(info.get('title', 'No Title'))
        super().__init__({'name': f"{title}.html"})
        self.info_data = info 

    def get(self):
        return self.make_html()

    def make_html(self):
        data = self.info_data
        url_map = data.get('url_map', {})
        content = data['content']

        # 문자열 치환
        for original, filename in url_map.items():
            if len(original) > 10:
                content = content.replace(original, filename)
            
            # 해시값 치환
            if len(original) > 20 and '/' not in original:
                pattern = re.compile(r'src="[^"]*?' + re.escape(original) + r'[^"]*"')
                content = pattern.sub(f'src="{filename}"', content)

        # DOM 정리
        try:
            soup = Soup(content)
            for img in soup.find_all('img'):
                src = img.get('src', '')
                if not src.startswith('http'):
                    for attr in ['srcset', 'loading', 'style', 'width', 'height', 'class', 'data-originalurl']:
                        if img.has_attr(attr): del img[attr]
                    img['style'] = "max-width: 100%; height: auto; display: block; margin: 20px auto; border-radius: 8px;"
            
            for tag in soup.find_all(['video', 'script', 'iframe', 'style']):
                tag.decompose()
            for a in soup.find_all('a'): a['target'] = '_blank'
            
            content = str(soup)
        except: pass

        # 댓글 HTML
        comments_html = ""
        if data['comments']:
            for c in data['comments']:
                initial = c['writer'][0] if c['writer'] else "?"
                indent_style = "margin-left: 2rem; border-left: 2px solid #3f3f46; padding-left: 0.5rem;" if c.get('parent_id') else ""
                
                comments_html += f"""
                <div class="comment-item" style="{indent_style}">
                    <div class="comment-avatar">{initial}</div>
                    <div class="comment-body">
                        <div class="comment-meta">
                            <span class="comment-author">{c['writer']}</span>
                            <span class="comment-date">{c['date']}</span>
                        </div>
                        <div class="comment-text">{c['content']}</div>
                    </div>
                </div>
                """
        else:
            comments_html = "<div style='color:#777; text-align:center; padding:20px;'>댓글이 없습니다.</div>"

        # HTML
        html = f"""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>{data['title']}</title>
            <style>{KONE_CLEAN_CSS}</style>
        </head>
        <body>
            <div class="layout">
                <header>
                    <span class="badge">Kone</span>
                    <h1>{data['title']}</h1>
                    <div class="meta">
                        <div class="avatar">{data['writer'][0] if data['writer'] else '?'}</div>
                        <span>{data['writer']}</span>
                        <span>•</span>
                        <span>{data['date']}</span>
                        <span>•</span>
                        <a href="{data['url']}" target="_blank">원본 링크</a>
                    </div>
                </header>
                
                <div class="prose">
                    {content}
                </div>

                <div class="comments-section">
                    <div class="comments-header">
                        댓글 {len(data['comments'])}
                    </div>
                    {comments_html}
                </div>
            </div>
        </body>
        </html>
        """
        
        f = BytesIO()
        f.write(html.encode('utf-8'))
        f.seek(0)
        return {'url': f}

messageBox('{}: kone (Debug Ver)'.format(tr_('사이트를 추가했습니다')))